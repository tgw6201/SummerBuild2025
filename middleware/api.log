2025-06-14 17:50:55,190 - middleware.recipe - DEBUG - Loading .env file
2025-06-14 17:50:55,198 - middleware.recipe - DEBUG - Initializing OpenAI model
2025-06-14 17:51:25,097 - middleware.recipe - INFO - Received request with input: {'user': {'age': 30, 'food_preferences': {'allergies': ['peanuts', 'shellfish'], 'special_diet': 'vegetarian'}, 'calorie_target': 2000}, 'ingredients': [{'food': 'whole wheat flour', 'quantity': 1.5, 'unit': 'cups'}, {'food': 'brown sugar', 'quantity': 0.25, 'unit': 'cup'}, {'food': 'olive oil', 'quantity': 2.0, 'unit': 'tablespoons'}, {'food': 'eggs', 'quantity': 1.0, 'unit': 'count'}, {'food': 'almond milk', 'quantity': 0.75, 'unit': 'cup'}, {'food': 'baking soda', 'quantity': 1.0, 'unit': 'teaspoon'}, {'food': 'blueberries', 'quantity': 1.0, 'unit': 'cup'}]}
2025-06-14 17:51:25,097 - middleware.recipe - DEBUG - Searching Spoonacular for ingredient: whole wheat flour
2025-06-14 17:51:25,101 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:25,738 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/search?apiKey=f9e0ebcacc7d4548838c820f298a1339&query=whole%20wheat%20flour HTTP/1.1" 200 None
2025-06-14 17:51:25,746 - middleware.recipe - DEBUG - Fetching nutrition details for ingredient ID: 20080
2025-06-14 17:51:25,747 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:26,319 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/20080/information?apiKey=f9e0ebcacc7d4548838c820f298a1339&amount=1.5&unit=cups HTTP/1.1" 200 None
2025-06-14 17:51:26,320 - middleware.recipe - DEBUG - Searching Spoonacular for ingredient: brown sugar
2025-06-14 17:51:26,321 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:26,859 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/search?apiKey=f9e0ebcacc7d4548838c820f298a1339&query=brown%20sugar HTTP/1.1" 200 None
2025-06-14 17:51:26,861 - middleware.recipe - DEBUG - Fetching nutrition details for ingredient ID: 19334
2025-06-14 17:51:26,861 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:27,429 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/19334/information?apiKey=f9e0ebcacc7d4548838c820f298a1339&amount=0.25&unit=cup HTTP/1.1" 200 None
2025-06-14 17:51:27,431 - middleware.recipe - DEBUG - Searching Spoonacular for ingredient: olive oil
2025-06-14 17:51:27,433 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:28,005 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/search?apiKey=f9e0ebcacc7d4548838c820f298a1339&query=olive%20oil HTTP/1.1" 200 None
2025-06-14 17:51:28,005 - middleware.recipe - DEBUG - Fetching nutrition details for ingredient ID: 4053
2025-06-14 17:51:28,006 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:28,582 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/4053/information?apiKey=f9e0ebcacc7d4548838c820f298a1339&amount=2.0&unit=tablespoons HTTP/1.1" 200 None
2025-06-14 17:51:28,584 - middleware.recipe - DEBUG - Searching Spoonacular for ingredient: eggs
2025-06-14 17:51:28,586 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:29,181 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/search?apiKey=f9e0ebcacc7d4548838c820f298a1339&query=eggs HTTP/1.1" 200 None
2025-06-14 17:51:29,182 - middleware.recipe - DEBUG - Fetching nutrition details for ingredient ID: 1123
2025-06-14 17:51:29,183 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:29,824 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/1123/information?apiKey=f9e0ebcacc7d4548838c820f298a1339&amount=1.0&unit=count HTTP/1.1" 200 None
2025-06-14 17:51:29,826 - middleware.recipe - DEBUG - Searching Spoonacular for ingredient: almond milk
2025-06-14 17:51:29,829 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:30,407 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/search?apiKey=f9e0ebcacc7d4548838c820f298a1339&query=almond%20milk HTTP/1.1" 200 None
2025-06-14 17:51:30,408 - middleware.recipe - DEBUG - Fetching nutrition details for ingredient ID: 93607
2025-06-14 17:51:30,409 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:30,626 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/93607/information?apiKey=f9e0ebcacc7d4548838c820f298a1339&amount=0.75&unit=cup HTTP/1.1" 200 None
2025-06-14 17:51:30,627 - middleware.recipe - DEBUG - Searching Spoonacular for ingredient: baking soda
2025-06-14 17:51:30,628 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:31,172 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/search?apiKey=f9e0ebcacc7d4548838c820f298a1339&query=baking%20soda HTTP/1.1" 200 None
2025-06-14 17:51:31,173 - middleware.recipe - DEBUG - Fetching nutrition details for ingredient ID: 18372
2025-06-14 17:51:31,174 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:31,749 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/18372/information?apiKey=f9e0ebcacc7d4548838c820f298a1339&amount=1.0&unit=teaspoon HTTP/1.1" 200 None
2025-06-14 17:51:31,750 - middleware.recipe - DEBUG - Searching Spoonacular for ingredient: blueberries
2025-06-14 17:51:31,750 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:32,305 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/search?apiKey=f9e0ebcacc7d4548838c820f298a1339&query=blueberries HTTP/1.1" 200 None
2025-06-14 17:51:32,306 - middleware.recipe - DEBUG - Fetching nutrition details for ingredient ID: 9050
2025-06-14 17:51:32,306 - urllib3.connectionpool - DEBUG - Starting new HTTPS connection (1): api.spoonacular.com:443
2025-06-14 17:51:32,904 - urllib3.connectionpool - DEBUG - https://api.spoonacular.com:443 "GET /food/ingredients/9050/information?apiKey=f9e0ebcacc7d4548838c820f298a1339&amount=1.0&unit=cup HTTP/1.1" 200 None
2025-06-14 17:51:32,905 - middleware.recipe - INFO - Total calories: 1180.56, Status: within target
2025-06-14 17:51:32,905 - middleware.recipe - DEBUG - Generating recipe with LangChain
2025-06-14 17:51:32,949 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fcc66bcd-b9fa-4d42-9d29-062673a1d047', 'json_data': {'messages': [{'content': "Suggest a recipe for a 30-year-old user with these ingredients: 1.5 cups whole wheat flour, 0.25 cup brown sugar, 2.0 tablespoons olive oil, 1.0 count eggs, 0.75 cup almond milk, 1.0 teaspoon baking soda, 1.0 cup blueberries. \n                The user has allergies to peanuts, shellfish, follows a vegetarian diet, and has a daily calorie target of 2000 kcal. \n                Return the recipe in JSON format with 'name', 'ingredients', and 'instructions', ensuring it aligns with the diet and avoids allergens.", 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.7}}
2025-06-14 17:51:32,950 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-06-14 17:51:32,955 - httpcore.connection - DEBUG - connect_tcp.started host='api.openai.com' port=443 local_address=None timeout=None socket_options=None
2025-06-14 17:51:32,973 - httpcore.connection - DEBUG - connect_tcp.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000176DEE93E00>
2025-06-14 17:51:32,974 - httpcore.connection - DEBUG - start_tls.started ssl_context=<ssl.SSLContext object at 0x00000176DEC3A720> server_hostname='api.openai.com' timeout=None
2025-06-14 17:51:32,992 - httpcore.connection - DEBUG - start_tls.complete return_value=<httpcore._backends.anyio.AnyIOStream object at 0x00000176DEE4E850>
2025-06-14 17:51:32,993 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-14 17:51:32,994 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-14 17:51:32,994 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-14 17:51:32,995 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-14 17:51:32,995 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-14 17:51:33,596 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 14 Jun 2025 09:51:35 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'337'), (b'Connection', b'keep-alive'), (b'vary', b'Origin'), (b'x-request-id', b'req_5f4ba3885032dbc8f67d86c6753c8fd4'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'Set-Cookie', b'__cf_bm=E5L3MtPP1wmx868U.DivyM7juaxzV4fhTV_NUn9IVI0-1749894695-1.0.1.1-TP0cuhW2GMoJh3QNUNBhoXGRQHUl27OL9SN88KmEPgX40dvHr5Sx1.4JLnEdW4YPBnMFtizS.XxcT14A0_vXgXUqDyUAQcAIW0Mu0KeZuQA; path=/; expires=Sat, 14-Jun-25 10:21:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'X-Content-Type-Options', b'nosniff'), (b'Set-Cookie', b'_cfuvid=BnWF7hueUoBEzntjDS.2rGkyvEkdDZ68zIgyXzYQcrM-1749894695808-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94f8ee151e86406e-SIN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-14 17:51:33,597 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-06-14 17:51:33,597 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-14 17:51:33,597 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-14 17:51:33,598 - httpcore.http11 - DEBUG - response_closed.started
2025-06-14 17:51:33,598 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-14 17:51:33,598 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "429 Too Many Requests" Headers([('date', 'Sat, 14 Jun 2025 09:51:35 GMT'), ('content-type', 'application/json; charset=utf-8'), ('content-length', '337'), ('connection', 'keep-alive'), ('vary', 'Origin'), ('x-request-id', 'req_5f4ba3885032dbc8f67d86c6753c8fd4'), ('strict-transport-security', 'max-age=31536000; includeSubDomains; preload'), ('cf-cache-status', 'DYNAMIC'), ('set-cookie', '__cf_bm=E5L3MtPP1wmx868U.DivyM7juaxzV4fhTV_NUn9IVI0-1749894695-1.0.1.1-TP0cuhW2GMoJh3QNUNBhoXGRQHUl27OL9SN88KmEPgX40dvHr5Sx1.4JLnEdW4YPBnMFtizS.XxcT14A0_vXgXUqDyUAQcAIW0Mu0KeZuQA; path=/; expires=Sat, 14-Jun-25 10:21:35 GMT; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('x-content-type-options', 'nosniff'), ('set-cookie', '_cfuvid=BnWF7hueUoBEzntjDS.2rGkyvEkdDZ68zIgyXzYQcrM-1749894695808-0.0.1.1-604800000; path=/; domain=.api.openai.com; HttpOnly; Secure; SameSite=None'), ('server', 'cloudflare'), ('cf-ray', '94f8ee151e86406e-SIN'), ('alt-svc', 'h3=":443"; ma=86400')])
2025-06-14 17:51:33,599 - openai._base_client - DEBUG - request_id: req_5f4ba3885032dbc8f67d86c6753c8fd4
2025-06-14 17:51:33,599 - openai._base_client - DEBUG - Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\_base_client.py", line 1535, in request
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2025-06-14 17:51:33,630 - openai._base_client - DEBUG - Retrying due to status code 429
2025-06-14 17:51:33,630 - openai._base_client - DEBUG - 2 retries left
2025-06-14 17:51:33,630 - openai._base_client - INFO - Retrying request to /chat/completions in 0.435645 seconds
2025-06-14 17:51:34,069 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fcc66bcd-b9fa-4d42-9d29-062673a1d047', 'json_data': {'messages': [{'content': "Suggest a recipe for a 30-year-old user with these ingredients: 1.5 cups whole wheat flour, 0.25 cup brown sugar, 2.0 tablespoons olive oil, 1.0 count eggs, 0.75 cup almond milk, 1.0 teaspoon baking soda, 1.0 cup blueberries. \n                The user has allergies to peanuts, shellfish, follows a vegetarian diet, and has a daily calorie target of 2000 kcal. \n                Return the recipe in JSON format with 'name', 'ingredients', and 'instructions', ensuring it aligns with the diet and avoids allergens.", 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.7}}
2025-06-14 17:51:34,071 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-06-14 17:51:34,072 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-14 17:51:34,073 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-14 17:51:34,074 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-14 17:51:34,075 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-14 17:51:34,075 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-14 17:51:35,143 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 14 Jun 2025 09:51:37 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'337'), (b'Connection', b'keep-alive'), (b'vary', b'Origin'), (b'x-request-id', b'req_2ace30e87a80a81907932aa226702453'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94f8ee1bdb6a406e-SIN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-14 17:51:35,143 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-06-14 17:51:35,144 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-14 17:51:35,144 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-14 17:51:35,144 - httpcore.http11 - DEBUG - response_closed.started
2025-06-14 17:51:35,144 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-14 17:51:35,145 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 14 Jun 2025 09:51:37 GMT', 'content-type': 'application/json; charset=utf-8', 'content-length': '337', 'connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': 'req_2ace30e87a80a81907932aa226702453', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '94f8ee1bdb6a406e-SIN', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-14 17:51:35,145 - openai._base_client - DEBUG - request_id: req_2ace30e87a80a81907932aa226702453
2025-06-14 17:51:35,145 - openai._base_client - DEBUG - Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\_base_client.py", line 1535, in request
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2025-06-14 17:51:35,146 - openai._base_client - DEBUG - Retrying due to status code 429
2025-06-14 17:51:35,146 - openai._base_client - DEBUG - 1 retry left
2025-06-14 17:51:35,146 - openai._base_client - INFO - Retrying request to /chat/completions in 0.936332 seconds
2025-06-14 17:51:36,088 - openai._base_client - DEBUG - Request options: {'method': 'post', 'url': '/chat/completions', 'files': None, 'idempotency_key': 'stainless-python-retry-fcc66bcd-b9fa-4d42-9d29-062673a1d047', 'json_data': {'messages': [{'content': "Suggest a recipe for a 30-year-old user with these ingredients: 1.5 cups whole wheat flour, 0.25 cup brown sugar, 2.0 tablespoons olive oil, 1.0 count eggs, 0.75 cup almond milk, 1.0 teaspoon baking soda, 1.0 cup blueberries. \n                The user has allergies to peanuts, shellfish, follows a vegetarian diet, and has a daily calorie target of 2000 kcal. \n                Return the recipe in JSON format with 'name', 'ingredients', and 'instructions', ensuring it aligns with the diet and avoids allergens.", 'role': 'user'}], 'model': 'gpt-3.5-turbo', 'stream': False, 'temperature': 0.7}}
2025-06-14 17:51:36,089 - openai._base_client - DEBUG - Sending HTTP Request: POST https://api.openai.com/v1/chat/completions
2025-06-14 17:51:36,090 - httpcore.http11 - DEBUG - send_request_headers.started request=<Request [b'POST']>
2025-06-14 17:51:36,090 - httpcore.http11 - DEBUG - send_request_headers.complete
2025-06-14 17:51:36,091 - httpcore.http11 - DEBUG - send_request_body.started request=<Request [b'POST']>
2025-06-14 17:51:36,091 - httpcore.http11 - DEBUG - send_request_body.complete
2025-06-14 17:51:36,091 - httpcore.http11 - DEBUG - receive_response_headers.started request=<Request [b'POST']>
2025-06-14 17:51:36,357 - httpcore.http11 - DEBUG - receive_response_headers.complete return_value=(b'HTTP/1.1', 429, b'Too Many Requests', [(b'Date', b'Sat, 14 Jun 2025 09:51:38 GMT'), (b'Content-Type', b'application/json; charset=utf-8'), (b'Content-Length', b'337'), (b'Connection', b'keep-alive'), (b'vary', b'Origin'), (b'x-request-id', b'req_31be828d945e0bbf31fb4afe3e27c956'), (b'strict-transport-security', b'max-age=31536000; includeSubDomains; preload'), (b'cf-cache-status', b'DYNAMIC'), (b'X-Content-Type-Options', b'nosniff'), (b'Server', b'cloudflare'), (b'CF-RAY', b'94f8ee286d0f406e-SIN'), (b'alt-svc', b'h3=":443"; ma=86400')])
2025-06-14 17:51:36,357 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions "HTTP/1.1 429 Too Many Requests"
2025-06-14 17:51:36,358 - httpcore.http11 - DEBUG - receive_response_body.started request=<Request [b'POST']>
2025-06-14 17:51:36,358 - httpcore.http11 - DEBUG - receive_response_body.complete
2025-06-14 17:51:36,358 - httpcore.http11 - DEBUG - response_closed.started
2025-06-14 17:51:36,359 - httpcore.http11 - DEBUG - response_closed.complete
2025-06-14 17:51:36,359 - openai._base_client - DEBUG - HTTP Response: POST https://api.openai.com/v1/chat/completions "429 Too Many Requests" Headers({'date': 'Sat, 14 Jun 2025 09:51:38 GMT', 'content-type': 'application/json; charset=utf-8', 'content-length': '337', 'connection': 'keep-alive', 'vary': 'Origin', 'x-request-id': 'req_31be828d945e0bbf31fb4afe3e27c956', 'strict-transport-security': 'max-age=31536000; includeSubDomains; preload', 'cf-cache-status': 'DYNAMIC', 'x-content-type-options': 'nosniff', 'server': 'cloudflare', 'cf-ray': '94f8ee286d0f406e-SIN', 'alt-svc': 'h3=":443"; ma=86400'})
2025-06-14 17:51:36,360 - openai._base_client - DEBUG - request_id: req_31be828d945e0bbf31fb4afe3e27c956
2025-06-14 17:51:36,360 - openai._base_client - DEBUG - Encountered httpx.HTTPStatusError
Traceback (most recent call last):
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\_base_client.py", line 1535, in request
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Client error '429 Too Many Requests' for url 'https://api.openai.com/v1/chat/completions'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429
2025-06-14 17:51:36,361 - openai._base_client - DEBUG - Re-raising status error
2025-06-14 17:51:36,363 - middleware.recipe - ERROR - Recipe generation error: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
Traceback (most recent call last):
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\recipe.py", line 154, in analyze_food
    recipe_response = await model.ainvoke(prompt_text)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 394, in ainvoke
    llm_result = await self.agenerate_prompt(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<8 lines>...
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 968, in agenerate_prompt
    return await self.agenerate(
           ^^^^^^^^^^^^^^^^^^^^^
        prompt_messages, stop=stop, callbacks=callbacks, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 926, in agenerate
    raise exceptions[0]
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1094, in _agenerate_with_cache
    result = await self._agenerate(
             ^^^^^^^^^^^^^^^^^^^^^^
        messages, stop=stop, run_manager=run_manager, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_openai\chat_models\base.py", line 1199, in _agenerate
    response = await self.async_client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<45 lines>...
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\_base_client.py", line 1748, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\_base_client.py", line 1555, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
2025-06-14 17:51:36,392 - middleware.recipe - ERROR - Error processing request: 500: Failed to generate recipe: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
Traceback (most recent call last):
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\recipe.py", line 154, in analyze_food
    recipe_response = await model.ainvoke(prompt_text)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 394, in ainvoke
    llm_result = await self.agenerate_prompt(
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<8 lines>...
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 968, in agenerate_prompt
    return await self.agenerate(
           ^^^^^^^^^^^^^^^^^^^^^
        prompt_messages, stop=stop, callbacks=callbacks, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 926, in agenerate
    raise exceptions[0]
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_core\language_models\chat_models.py", line 1094, in _agenerate_with_cache
    result = await self._agenerate(
             ^^^^^^^^^^^^^^^^^^^^^^
        messages, stop=stop, run_manager=run_manager, **kwargs
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\langchain_openai\chat_models\base.py", line 1199, in _agenerate
    response = await self.async_client.create(**payload)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\resources\chat\completions\completions.py", line 2028, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
    ...<45 lines>...
    )
    ^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\_base_client.py", line 1748, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\venv\Lib\site-packages\openai\_base_client.py", line 1555, in request
    raise self._make_status_error_from_response(err.response) from None
openai.RateLimitError: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\lohc4\Documents\GitHub\SummerBuild2025\middleware\recipe.py", line 161, in analyze_food
    raise HTTPException(status_code=500, detail=f"Failed to generate recipe: {str(e)}")
fastapi.exceptions.HTTPException: 500: Failed to generate recipe: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}
2025-06-14 19:35:30,605 - middleware.recipe - DEBUG - Loading .env file
2025-06-14 19:35:30,607 - middleware.recipe - DEBUG - Initializing OpenAI model
2025-06-14 19:35:53,015 - middleware.recipe - DEBUG - Loading .env file
2025-06-14 19:35:53,017 - middleware.recipe - DEBUG - Initializing OpenAI model
